# robots.txt
特定のウェブクローラに対して、クロールしても良いページやクロールしてはいけないページなどの指示をするファイル。 

例えば、Webアプリにおいて、  
「アプリのページは Google検索の結果に出していいよ、でも管理人用ページは検索結果に出さないで」のような指示ができる。
***

# 作り方
## robots.txtファイルを作る
アプリの publicディレクトリに robots.txtという名前のファイルを作る。
***

## robots.txt編集
以下の場合、すべてのページをクロールして OKという意味になる。
~~~
[public/robots.txt]

User-agent: *
Disallow:
~~~
##### User-agent: *
全てのウェブクローラを表す。
***

##### Disallow:
この配下に表示したくないページのディレクトリなどを指定する。
~~~
[public/robots.txt]

User-agent: *
Disallow: /admin/
~~~
この書き方だと、adminフォルダ配下のページはクロールしないように指示している。
***

# ❓ ログインが必要なページも Disallowに書いたほうがいい？？
結論、書かなくてOK!  
なぜなら、クローラーはログインが必要なページにはアクセスできないので、  
そもそもインデックス(検索エンジンがウェブページを収集してデータベースに登録すること)されることはないから。

ただし、クローラーがログイン不要のページからログイン必要なページに誤ってたどり着けるようなリンクがある場合は、    
そのページを Disallowで指定してクローラーがアクセスしないようにするのがいいかもしれない。
***

# Google Search Consoleに登録


